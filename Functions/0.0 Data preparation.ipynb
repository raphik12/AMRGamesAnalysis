{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0.0 Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation\n",
    "\n",
    "Imports libraries\n",
    "\n",
    "Loads data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "print(\"0.0 data preparation\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.transforms as tr\n",
    "import seaborn as sns\n",
    "import re\n",
    "import json\n",
    "import datetime\n",
    "import random\n",
    "import os\n",
    "\n",
    "from random import randint\n",
    "from ipywidgets import FloatProgress,IntProgress,IntText,Text,interact,interactive,IntSlider,FloatSlider\n",
    "from IPython.display import display\n",
    "from itertools import chain\n",
    "from scipy.stats import ttest_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### common variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# questions & columns\n",
    "indexColumn = 'index'\n",
    "emailQuestion = 'Email'\n",
    "timestampQuestion = 'Timestamp'\n",
    "idQuestion = 'ID'\n",
    "nameQuestion = 'Name'\n",
    "gameQuestion = 'Which game have you just played?'\n",
    "remarksQuestion = '12. Write here your remarks about the game: which feature was missing, what failed or glitched, what was great:'\n",
    "questionCount = 12\n",
    "firstLikertQuestionIndex = 0\n",
    "lastLikertQuestionIndex = 11\n",
    "firstSUSQuestionIndex = 0\n",
    "lastSUSQuestionIndex = 10\n",
    "questionArrayInt = [i+1 for i in range(questionCount)]\n",
    "questionArrayStr = sorted([\"Q\" + \"{0:0=2d}\".format(i) for i in questionArrayInt])\n",
    "#questionArrayStr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tryCreateFolder(path, displayMessage = False):\n",
    "    try:  \n",
    "        os.mkdir(path)\n",
    "    except OSError:  \n",
    "        if displayMessage:\n",
    "            print (\"Creation of the directory %s failed\" % path)\n",
    "    else:  \n",
    "        if displayMessage:\n",
    "            print (\"Successfully created the directory %s \" % path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the name of the directory to be created\n",
    "graphsSavePathStem = \"../Graphs\"\n",
    "tryCreateFolder(graphsSavePathStem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffixSUSNormalized = \"SUSNormalized\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateTestPath(number):\n",
    "    return dataFolderPath + testStem + str(number) + csvSuffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFolderPath = \"../Data/\"\n",
    "unanonymizedDataFolderPath = dataFolderPath + \"UnanonymizedData/\"\n",
    "csvSuffix = '.csv'\n",
    "rawSuffix = \"_event_raw\"\n",
    "rawAnonymizedSuffix = \"_event_raw_unanonymized\"\n",
    "dataStem = \"_data\"\n",
    "SUSNormalizedSuffix = \"_SUSNormalized\"\n",
    "\n",
    "testStem = \"test\"\n",
    "\n",
    "# Miranda House data\n",
    "dateStemMirandaHouse                       = \"2019-06-03\"\n",
    "unanonymizedDataFilesNamesStemMirandaHouse = dateStemMirandaHouse + rawAnonymizedSuffix\n",
    "dataFilesNamesStemMirandaHouse             = dateStemMirandaHouse + rawSuffix\n",
    "unanonymizedRaw20190603PathMirandaHouse    = unanonymizedDataFolderPath + unanonymizedDataFilesNamesStemMirandaHouse            + csvSuffix\n",
    "raw20190603PathMirandaHouse                = dataFolderPath             + dataFilesNamesStemMirandaHouse                        + csvSuffix\n",
    "data20190603Path                           = dataFolderPath             + dateStemMirandaHouse + dataStem                       + csvSuffix\n",
    "data20190603SUSNormalizedPath              = dataFolderPath             + dateStemMirandaHouse + dataStem + SUSNormalizedSuffix + csvSuffix\n",
    "\n",
    "# Cit√© des Sciences data\n",
    "dateStemCiteDesSciences                       = \"2019-07-03\"\n",
    "unanonymizedDataFilesNamesStemCiteDesSciences = dateStemCiteDesSciences + rawAnonymizedSuffix\n",
    "dataFilesNamesStemCiteDesSciences             = dateStemCiteDesSciences + rawSuffix\n",
    "unanonymizedRaw20190703PathCiteDesSciences    = unanonymizedDataFolderPath + unanonymizedDataFilesNamesStemCiteDesSciences            + csvSuffix\n",
    "raw20190703PathCiteDesSciences                = dataFolderPath             + dataFilesNamesStemCiteDesSciences                        + csvSuffix\n",
    "data20190703Path                              = dataFolderPath             + dateStemCiteDesSciences + dataStem                       + csvSuffix\n",
    "data20190703SUSNormalizedPath                 = dataFolderPath             + dateStemCiteDesSciences + dataStem + SUSNormalizedSuffix + csvSuffix\n",
    "\n",
    "# Miranda House 2 data\n",
    "dateStemMirandaHouse2                      = \"2019-08-02\"\n",
    "unanonymizedDataFilesNamesStemMirandaHouse2= dateStemMirandaHouse2 + rawAnonymizedSuffix\n",
    "dataFilesNamesStemMirandaHouse2            = dateStemMirandaHouse2 + rawSuffix\n",
    "unanonymizedRaw20190802PathMirandaHouse2   = unanonymizedDataFolderPath + unanonymizedDataFilesNamesStemMirandaHouse2            + csvSuffix\n",
    "raw20190802PathMirandaHouse2               = dataFolderPath             + dataFilesNamesStemMirandaHouse2                        + csvSuffix\n",
    "data20190802Path                           = dataFolderPath             + dateStemMirandaHouse2 + dataStem                       + csvSuffix\n",
    "data20190802SUSNormalizedPath              = dataFolderPath             + dateStemMirandaHouse2 + dataStem + SUSNormalizedSuffix + csvSuffix\n",
    "\n",
    "# Kigali data\n",
    "dateStemKigali                       = \"2019-08-28\"\n",
    "unanonymizedDataFilesNamesStemKigali = dateStemKigali + rawAnonymizedSuffix\n",
    "dataFilesNamesStemKigali             = dateStemKigali + rawSuffix\n",
    "unanonymizedRaw20190828PathKigali    = unanonymizedDataFolderPath + unanonymizedDataFilesNamesStemKigali            + csvSuffix\n",
    "raw20190828PathKigali                = dataFolderPath             + dataFilesNamesStemKigali                        + csvSuffix\n",
    "data20190828Path                     = dataFolderPath             + dateStemKigali + dataStem                       + csvSuffix\n",
    "data20190828SUSNormalizedPath        = dataFolderPath             + dateStemKigali + dataStem + SUSNormalizedSuffix + csvSuffix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data anonymization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning: works in-place\n",
    "def fixData(dataToFix):\n",
    "    questionsInvolved = [gameQuestion]\n",
    "    wrongSpelling = \"Fungal Invasion\"\n",
    "    correctSpelling = \"Fungal Invaders\"\n",
    "    for answer in dataToFix.index:\n",
    "        for question in questionsInvolved:\n",
    "            if dataToFix.loc[answer, question] == wrongSpelling:\n",
    "                dataToFix.loc[answer, question] = correctSpelling\n",
    "    return dataToFix\n",
    "        \n",
    "\n",
    "#testData = raw20190603.copy()\n",
    "#fixData(testData)\n",
    "#testData.loc[:, questionsInvolved]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixRawData(rawPath):\n",
    "    raw = pd.read_csv(rawPath, index_col=indexColumn, dtype=str)\n",
    "    fixData(raw)\n",
    "    raw.to_csv(rawPath, encoding='utf-8')\n",
    "    \n",
    "#fixRawData(raw20190603PathMirandaHouse)\n",
    "#fixRawData(raw20190703PathCiteDesSciences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns anonymized data: replaces Name by identifier and removes the Email column\n",
    "# reads from the unanomizedresult to the anonymizedPath\n",
    "def preprocessAnonymize(unanonymizedPath, anonymizedPath):\n",
    "    \n",
    "    # TODO filter out tests by our team members and organizers\n",
    "    \n",
    "    unanonymizedData = pd.read_csv(unanonymizedPath,dtype=str)\n",
    "\n",
    "    unanonymizedData = unanonymizedData.drop(axis=1, columns=[emailQuestion])\n",
    "\n",
    "    nameSet = set(unanonymizedData[nameQuestion])\n",
    "    nameSeries = pd.Series(list(nameSet))\n",
    "    for answerIndex in unanonymizedData.index:\n",
    "        name = unanonymizedData.loc[answerIndex, nameQuestion]\n",
    "        unanonymizedData.loc[answerIndex, nameQuestion] = nameSeries.index[nameSeries == name][0]\n",
    "    unanonymizedData.rename(columns={nameQuestion:idQuestion}, inplace=True)\n",
    "\n",
    "    fixData(unanonymizedData)\n",
    "        \n",
    "    if indexColumn in unanonymizedData.columns:\n",
    "        unanonymizedData = unanonymizedData.set_index(indexColumn)\n",
    "        \n",
    "    print(\"writing to \" + anonymizedPath)\n",
    "    unanonymizedData.to_csv(anonymizedPath, encoding='utf-8')\n",
    "    \n",
    "    return unanonymizedData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python adds an artefact index column with indices 0,..,n _by default_ when reading then writing a csv without using the argument index_col=.\n",
    "# This function fixes the consequences of forgetting to use this argument.\n",
    "def getRidOfArtefactFirstColumn(rawPath):\n",
    "    raw = pd.read_csv(rawPath,dtype=str)\n",
    "    raw = raw.drop(axis=1, columns=raw.columns[0])\n",
    "    raw = raw.set_index('Timestamp')\n",
    "    raw.to_csv(rawPath, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doAnonymize = False\n",
    "\n",
    "if doAnonymize:\n",
    "    raw20190603 = preprocessAnonymize(\\\n",
    "        unanonymizedRaw20190603PathMirandaHouse,\\\n",
    "        raw20190603PathMirandaHouse)\n",
    "    \n",
    "if doAnonymize:\n",
    "    raw20190703 = preprocessAnonymize(\\\n",
    "        unanonymizedRaw20190703PathCiteDesSciences,\\\n",
    "        raw20190703PathCiteDesSciences)\n",
    "    \n",
    "if doAnonymize:\n",
    "    raw20190802 = preprocessAnonymize(\\\n",
    "        unanonymizedRaw20190802PathMirandaHouse2,\\\n",
    "        raw20190802PathMirandaHouse2)\n",
    "\n",
    "if doAnonymize:\n",
    "    # first merge the three separate data files\n",
    "    unanonymizedPath1 = \"../Data/UnanonymizedData/2019-08-28_website_Game-Questionnaire-Game1.csv\"\n",
    "    unanonymizedPath2 = \"../Data/UnanonymizedData/2019-08-28_website_Game-Questionnaire-Game2.csv\"\n",
    "    unanonymizedPath3 = \"../Data/UnanonymizedData/2019-08-28_website_Game-Questionnaire-Game3.csv\"\n",
    "\n",
    "    unanonymized1 = pd.read_csv(unanonymizedPath1,dtype=str)\n",
    "    unanonymized2 = pd.read_csv(unanonymizedPath2,dtype=str)\n",
    "    unanonymized3 = pd.read_csv(unanonymizedPath3,dtype=str)\n",
    "\n",
    "    concatenated = pd.concat([unanonymized1, unanonymized2, unanonymized3])\n",
    "\n",
    "    assert len(unanonymized1) + len(unanonymized2) + len(unanonymized3) == len(concatenated)\n",
    "\n",
    "    concatenated.index = range(len(concatenated.index))\n",
    "    concatenated.index.name = indexColumn\n",
    "    concatenated.to_csv(unanonymizedRaw20190828PathKigali, encoding='utf-8')\n",
    "    \n",
    "    raw20190828 = preprocessAnonymize(\\\n",
    "        unanonymizedRaw20190828PathKigali,\\\n",
    "        raw20190828PathKigali)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Miranda House\n",
    "try:\n",
    "    raw20190603  = pd.read_csv(raw20190603PathMirandaHouse, index_col=indexColumn, dtype=str)\n",
    "except:\n",
    "    print(\"Miranda House data read failed: processing raw data...\")\n",
    "    raw20190603 = preprocessAnonymize(unanonymizedRaw20190603PathMirandaHouse, raw20190603PathMirandaHouse)\n",
    "    raw20190603 = pd.read_csv(raw20190603PathMirandaHouse, index_col=indexColumn, dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cite des Sciences\n",
    "try:\n",
    "    raw20190703  = pd.read_csv(raw20190703PathCiteDesSciences, index_col=indexColumn, dtype=str)\n",
    "except:\n",
    "    print(\"Cite des Sciences data read failed: processing raw data...\")\n",
    "    raw20190703 = preprocessAnonymize(unanonymizedRaw20190703PathCiteDesSciences, raw20190703PathCiteDesSciences)\n",
    "    raw20190703 = pd.read_csv(raw20190703PathCiteDesSciences, index_col=indexColumn, dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Miranda House 2\n",
    "try:\n",
    "    raw20190802  = pd.read_csv(raw20190802PathMirandaHouse2, index_col=indexColumn, dtype=str)\n",
    "except:\n",
    "    print(\"Miranda House 2 data read failed: processing raw data...\")\n",
    "    raw20190802 = preprocessAnonymize(unanonymizedRaw20190802PathMirandaHouse2, raw20190802PathMirandaHouse2)\n",
    "    raw20190802 = pd.read_csv(raw20190802PathMirandaHouse2, index_col=indexColumn, dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kigali\n",
    "try:\n",
    "    raw20190828  = pd.read_csv(raw20190828PathKigali, index_col=indexColumn, dtype=str)\n",
    "except:\n",
    "    print(\"Kigali data read failed: processing raw data...\")\n",
    "    raw20190828 = preprocessAnonymize(unanonymizedRaw20190828PathKigali, raw20190828PathKigali)\n",
    "    raw20190828 = pd.read_csv(raw20190828PathKigali, index_col=indexColumn, dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "assert ((raw20190603.columns == raw20190703.columns).all()), \\\n",
    "(\"column mismatch: data from different experiments must have the same columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## additional treatment: column name extraction\n",
    "Game question\n",
    "short, indexed, Likert, SUS questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstQuestionIndex = len(raw20190603.columns)-len(questionArrayStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortQuestions = pd.Series(index=questionArrayStr, data=raw20190603.columns[firstQuestionIndex:])\n",
    "shortQuestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexedQuestions = pd.Series(index=range(1,13), data=raw20190603.columns[firstQuestionIndex:])\n",
    "indexedQuestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexedLikertQuestions = indexedQuestions[firstLikertQuestionIndex:lastLikertQuestionIndex]\n",
    "indexedLikertQuestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positiveLikertQuestions = indexedLikertQuestions.copy()\n",
    "positiveLikertQuestions = positiveLikertQuestions[positiveLikertQuestions.index % 2 == 1]\n",
    "positiveLikertQuestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negativeLikertQuestions = indexedLikertQuestions.copy()\n",
    "negativeLikertQuestions = negativeLikertQuestions[negativeLikertQuestions.index % 2 == 0]\n",
    "negativeLikertQuestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shortLikertQuestions = shortQuestions[firstLikertQuestionIndex:lastLikertQuestionIndex]\n",
    "shortLikertQuestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexedSUSQuestions = indexedQuestions[firstSUSQuestionIndex:lastSUSQuestionIndex]\n",
    "indexedSUSQuestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positiveSUSQuestions = indexedSUSQuestions.copy()\n",
    "positiveSUSQuestions = positiveSUSQuestions[positiveSUSQuestions.index % 2 == 1]\n",
    "positiveSUSQuestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negativeSUSQuestions = indexedSUSQuestions.copy()\n",
    "negativeSUSQuestions = negativeSUSQuestions[negativeSUSQuestions.index % 2 == 0]\n",
    "negativeSUSQuestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shortSUSQuestions = shortQuestions[firstSUSQuestionIndex:lastSUSQuestionIndex]\n",
    "shortSUSQuestions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-word question descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortDescQuestions = pd.Series(index=raw20190603.columns[firstQuestionIndex:], data=[\n",
    "     \"frequency\",\t\t\t\t#01. I think that I would like to play this game frequently.',\n",
    "     \"complexity\",\t\t\t\t#02. I found the game unnecessarily complex.',\n",
    "     \"ease\",\t\t\t\t\t#03. I thought the game was easy to play.',\n",
    "     \"need for support\",\t\t#04. I think that I would need the support of a technical person to be able to play this game.',    \n",
    "     \"integration\",\t\t\t\t#05. I found the various functions in this game were well integrated.',\n",
    "     \"consistency\",\t\t\t\t#06. I thought there was too much inconsistency in this game.',\n",
    "     \"others' learning\",\t\t#07. I would imagine that most people would learn to play this game very quickly.',\n",
    "     \"convenience\",\t\t\t\t#08. I found the game very cumbersome to play.',\n",
    "\t \"confidence\",\t\t\t\t#09. I felt very confident using the game.',\n",
    "\t \"my learning\",\t\t\t\t#10. I needed to learn a lot of things before I could get going with this game.',\n",
    "\t \"recommendation\",\t\t\t#11. I would recommend this game to a friend.',\n",
    "\t \"remarks\",\t\t\t\t\t#12. Write here your remarks about the game: which feature was missing, what failed or glitched, what was great:',\n",
    "\t ])\n",
    "shortDescQuestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortDescQuestions = pd.Series(index=raw20190603.columns[firstQuestionIndex:], data=[\n",
    "     \"replay appeal\",\t\t\t#01. I think that I would like to play this game frequently.',\n",
    "     \"simplicity\",\t\t\t\t#02. I found the game unnecessarily complex.',\n",
    "     \"ease\",\t\t\t\t\t#03. I thought the game was easy to play.',\n",
    "     \"autonomy\",\t\t        #04. I think that I would need the support of a technical person to be able to play this game.',    \n",
    "     \"integration\",\t\t\t\t#05. I found the various functions in this game were well integrated.',\n",
    "     \"consistency\",\t\t\t\t#06. I thought there was too much inconsistency in this game.',\n",
    "     \"learnable by others\",\t\t#07. I would imagine that most people would learn to play this game very quickly.',\n",
    "     \"convenience\",\t\t\t\t#08. I found the game very cumbersome to play.', maniability\n",
    "\t \"confidence\",\t\t\t\t#09. I felt very confident using the game.',\n",
    "\t \"learnable by self\",\t    #10. I needed to learn a lot of things before I could get going with this game.',\n",
    "\t \"recommendation\",\t\t\t#11. I would recommend this game to a friend.',\n",
    "\t \"remarks\",\t\t\t\t\t#12. Write here your remarks about the game: which feature was missing, what failed or glitched, what was great:',\n",
    "\t ])\n",
    "shortDescQuestions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## game names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games = raw20190603[gameQuestion].unique()\n",
    "games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gameDrBugTitle = 'Dr Bug: Microbe Mayhem'\n",
    "gameSuperbugsTitle = 'Superbugs: the Game'\n",
    "gameFungalTitle = 'Fungal Invaders'\n",
    "\n",
    "assert (set(games)==set((gameDrBugTitle, gameSuperbugsTitle, gameFungalTitle))), (\"Wrong list of games\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identityGameNames = pd.Series(index=[gameDrBugTitle, gameSuperbugsTitle, gameFungalTitle],\n",
    "                           data=[gameDrBugTitle, gameSuperbugsTitle, gameFungalTitle])\n",
    "shortGameNames = pd.Series(index=identityGameNames.index,\n",
    "                           data=['Dr Bug', 'Superbugs', 'Fungal Invaders'])\n",
    "shortGameNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getShortGameTitle(longGameName):\n",
    "    return shortGameNames.get(longGameName, \"unknown\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas & Python syntax tests\n",
    "Based on https://stackoverflow.com/questions/38309729/count-unique-values-with-pandas-per-groups/38309807"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    ''.join(['a','b','c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "\n",
    "    df = pd.DataFrame(index = range(9), columns=range(2))\n",
    "\n",
    "    df.loc[0,0]=0\n",
    "    df.loc[1,0]=1\n",
    "    df.loc[2,0]=1\n",
    "    df.loc[3,0]=2\n",
    "    df.loc[4,0]=2\n",
    "    df.loc[5,0]=2\n",
    "    df.loc[6,0]=3\n",
    "    df.loc[7,0]=3\n",
    "    df.loc[8,0]=3\n",
    "\n",
    "    df.loc[0,1]=0\n",
    "    df.loc[1,1]=0\n",
    "    df.loc[2,1]=1\n",
    "    df.loc[3,1]=0\n",
    "    df.loc[4,1]=1\n",
    "    df.loc[5,1]=2\n",
    "    df.loc[6,1]=0\n",
    "    df.loc[7,1]=1\n",
    "    df.loc[8,1]=1\n",
    "\n",
    "    print(df)\n",
    "    print()\n",
    "    print(df.groupby(by=0, as_index=False).agg({1: pd.Series.nunique}))\n",
    "    print()\n",
    "    print(df.groupby(0).count())\n",
    "    print()\n",
    "    print(df.groupby(0).nunique())\n",
    "    print()\n",
    "    print(df.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    testtotal = 0\n",
    "    for x in range(4):\n",
    "        print(\"x=\" + str(x))\n",
    "        if x % 2 == 0:\n",
    "            for y in range(4):\n",
    "                print(\"\\ty=\" + str(y))\n",
    "                if y == 2:\n",
    "                    print(\"\\t\\ty == 2\")\n",
    "                    break\n",
    "                testtotal = testtotal + 1\n",
    "                print(\"\\t\\telse: testtotal=\"+str(testtotal))\n",
    "    testtotal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## advanced treatment: data refinement\n",
    "Scores are stored as strings and must be converted to integers and NaNs for missing values\n",
    "\n",
    "### Removing the players who haven't played all of the games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw20190603.loc[:,[idQuestion,gameQuestion]].groupby(idQuestion).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should be the way to go\n",
    "if False:\n",
    "    groupByAgg = raw20190603.loc[:,[idQuestion,gameQuestion]].groupby(by=idQuestion, as_index=False).agg({gameQuestion: pd.Series.nunique})\n",
    "    groupByAgg.to_csv(\n",
    "        dataFolderPath + \"groupByAgg\" + csvSuffix,\n",
    "        encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "if False:\n",
    "    groupByBothCount = raw20190603.loc[:,[idQuestion,gameQuestion]].groupby([idQuestion,gameQuestion]).count()\n",
    "    groupByBothCount.to_csv(\n",
    "        dataFolderPath + \"groupByBothCount\" + csvSuffix,\n",
    "        encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    groupByIdCount = raw20190603.loc[:,[idQuestion,gameQuestion]].groupby(idQuestion).count()\n",
    "    groupByIdCount.to_csv(\n",
    "        dataFolderPath + \"groupByIdCount\" + csvSuffix,\n",
    "        encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    playedAllGamesIDs = groupByAgg[groupByAgg[gameQuestion] == 3][idQuestion].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPlayedAllGamesIDs(anonymizedData):\n",
    "    groupByAgg = anonymizedData.loc[:,[idQuestion,gameQuestion]].groupby(by=idQuestion, as_index=False).agg({gameQuestion: pd.Series.nunique})\n",
    "    return groupByAgg[groupByAgg[gameQuestion] == 3][idQuestion].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refining: filtering and computing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the \"break\" instruction will save some time but a participant who played twice will still appear twice in the respondent list\n",
    "def getNumericalData(anonymizedData):\n",
    "    numericalData = anonymizedData.copy()\n",
    "    idsToDrop = set()\n",
    "    playedAllGamesIDs = getPlayedAllGamesIDs(anonymizedData)\n",
    "\n",
    "    for respondent in numericalData.index:\n",
    "        if numericalData.loc[respondent, idQuestion] in playedAllGamesIDs:\n",
    "            for question in indexedLikertQuestions:\n",
    "                try:\n",
    "                    numericalData.loc[respondent, question] = int(numericalData.loc[respondent, question])\n",
    "                except:\n",
    "                    # option 1: use NaNs\n",
    "                    # numericalData.loc[respondent, question] = np.nan\n",
    "                    # option 2: exclude entry; make sure all data from this person are excluded for the sake of consistency.\n",
    "                    idsToDrop.add(numericalData.loc[respondent, idQuestion])\n",
    "                    break\n",
    "        else:\n",
    "            # print(\"id \" + str(numericalData.loc[respondent, idQuestion]) + \" did not play all the games\")\n",
    "            idsToDrop.add(numericalData.loc[respondent, idQuestion])\n",
    "    \n",
    "    # exclude participants who didn't answer some questions or didn't play some games\n",
    "    numericalData.drop(\n",
    "                        axis=0,\n",
    "                        inplace=True,\n",
    "                        index=numericalData[numericalData[idQuestion].isin(idsToDrop)].index)\n",
    "    \n",
    "    # exclude participants who didn't play all the games\n",
    "    # TODO\n",
    "\n",
    "\n",
    "    # reindex\n",
    "    numericalData.index = range(len(numericalData.index))\n",
    "    numericalData.index.name = indexColumn\n",
    "    \n",
    "    return numericalData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNormalizedNumericalData(numericalData):\n",
    "    # transforms the agreement scores (1-5 Likert scale) into a 0-4 mark with 0 = bad and 4 = great\n",
    "    normalizedNumericalData = numericalData.copy()\n",
    "    for respondent in normalizedNumericalData.index:\n",
    "        for question in indexedLikertQuestions:\n",
    "            answer = normalizedNumericalData.loc[respondent, question]\n",
    "            if pd.notna(answer):\n",
    "                if question in negativeLikertQuestions.values:            \n",
    "                    normalizedNumericalData.loc[respondent, question] = 5-answer\n",
    "                else:\n",
    "                    normalizedNumericalData.loc[respondent, question] = answer-1\n",
    "                    \n",
    "    return normalizedNumericalData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypeDict = dict()\n",
    "for question in indexedLikertQuestions:\n",
    "    dtypeDict[question] = \"int\"\n",
    "dtypeDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Miranda House data\n",
    "\n",
    "try:\n",
    "    data20190603               = pd.read_csv(data20190603Path, index_col=indexColumn, dtype=dtypeDict)\n",
    "    data20190603SUSNormalized  = pd.read_csv(data20190603SUSNormalizedPath, index_col=indexColumn, dtype=dtypeDict)\n",
    "except:\n",
    "    data20190603 = getNumericalData(raw20190603)\n",
    "    data20190603SUSNormalized = getNormalizedNumericalData(data20190603)\n",
    "    \n",
    "    data20190603.to_csv(data20190603Path, encoding='utf-8')\n",
    "    data20190603SUSNormalized.to_csv(data20190603SUSNormalizedPath, encoding='utf-8')\n",
    "    \n",
    "    data20190603               = pd.read_csv(data20190603Path, index_col=indexColumn, dtype=dtypeDict)\n",
    "    data20190603SUSNormalized  = pd.read_csv(data20190603SUSNormalizedPath, index_col=indexColumn, dtype=dtypeDict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cite des Sciences data\n",
    "\n",
    "try:\n",
    "    data20190703               = pd.read_csv(data20190703Path, index_col=indexColumn, dtype=dtypeDict)\n",
    "    data20190703SUSNormalized  = pd.read_csv(data20190703SUSNormalizedPath, index_col=indexColumn, dtype=dtypeDict)\n",
    "except:\n",
    "    data20190703 = getNumericalData(raw20190703)\n",
    "    data20190703SUSNormalized = getNormalizedNumericalData(data20190703)\n",
    "    \n",
    "    data20190703.to_csv(data20190703Path, encoding='utf-8')\n",
    "    data20190703SUSNormalized.to_csv(data20190703SUSNormalizedPath, encoding='utf-8')\n",
    "    \n",
    "    data20190703               = pd.read_csv(data20190703Path, index_col=indexColumn, dtype=dtypeDict)\n",
    "    data20190703SUSNormalized  = pd.read_csv(data20190703SUSNormalizedPath, index_col=indexColumn, dtype=dtypeDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Miranda House 2 data\n",
    "\n",
    "try:\n",
    "    data20190802               = pd.read_csv(data20190802Path, index_col=indexColumn, dtype=dtypeDict)\n",
    "    data20190802SUSNormalized  = pd.read_csv(data20190802SUSNormalizedPath, index_col=indexColumn, dtype=dtypeDict)\n",
    "except:\n",
    "    data20190802 = getNumericalData(raw20190802)\n",
    "    data20190802SUSNormalized = getNormalizedNumericalData(data20190802)\n",
    "    \n",
    "    data20190802.to_csv(data20190802Path, encoding='utf-8')\n",
    "    data20190802SUSNormalized.to_csv(data20190802SUSNormalizedPath, encoding='utf-8')\n",
    "    \n",
    "    data20190802               = pd.read_csv(data20190802Path, index_col=indexColumn, dtype=dtypeDict)\n",
    "    data20190802SUSNormalized  = pd.read_csv(data20190802SUSNormalizedPath, index_col=indexColumn, dtype=dtypeDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kigali data\n",
    "\n",
    "try:\n",
    "    data20190828               = pd.read_csv(data20190828Path, index_col=indexColumn, dtype=dtypeDict)\n",
    "    data20190828SUSNormalized  = pd.read_csv(data20190828SUSNormalizedPath, index_col=indexColumn, dtype=dtypeDict)\n",
    "except:\n",
    "    data20190828 = getNumericalData(raw20190828)\n",
    "    data20190828SUSNormalized = getNormalizedNumericalData(data20190828)\n",
    "    \n",
    "    data20190828.to_csv(data20190828Path, encoding='utf-8')\n",
    "    data20190828SUSNormalized.to_csv(data20190828SUSNormalizedPath, encoding='utf-8')\n",
    "    \n",
    "    data20190828               = pd.read_csv(data20190828Path, index_col=indexColumn, dtype=dtypeDict)\n",
    "    data20190828SUSNormalized  = pd.read_csv(data20190828SUSNormalizedPath, index_col=indexColumn, dtype=dtypeDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = dict()\n",
    "\n",
    "datasets[\"data20190603\"]              = data20190603\n",
    "datasets[\"data20190603SUSNormalized\"] = data20190603SUSNormalized\n",
    "datasets[\"data20190703\"]              = data20190703\n",
    "datasets[\"data20190703SUSNormalized\"] = data20190703SUSNormalized\n",
    "datasets[\"data20190802\"]              = data20190802\n",
    "datasets[\"data20190802SUSNormalized\"] = data20190802SUSNormalized\n",
    "datasets[\"data20190828\"]              = data20190828\n",
    "datasets[\"data20190828SUSNormalized\"] = data20190828SUSNormalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectDataset(datasetName):\n",
    "    #_dataName = \"data20190603\"\n",
    "    _dataName = datasetName\n",
    "    assert (_dataName in datasets), (\"Not found in datasets: '\" + _dataName + \"'\")\n",
    "    inputData=datasets[_dataName]\n",
    "\n",
    "    _dataNameSUSNormalized = _dataName + suffixSUSNormalized\n",
    "    assert (_dataName in datasets), (\"Not found in datasets: '\" + _dataNameSUSNormalized + \"'\")\n",
    "    inputDataSUSNormalized=datasets[_dataNameSUSNormalized]\n",
    "    \n",
    "    return _dataName, inputData, _dataNameSUSNormalized, inputDataSUSNormalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# digital5StepDescriptions = range(minLikertValue, maxLikertValue+1)\n",
    "# mixed5StepDecriptions = ['Strongly agree', '2', '3', '4', 'Strongly disagree']\n",
    "# likert5StepDescriptions = ['Strongly agree', 'Slightly agree', 'Neutral', 'Slightly disagree', 'Strongly disagree']\n",
    "likert5StepDescriptions = ['Strongly agree', 'Agree', 'Neutral', 'Disagree', 'Strongly disagree']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMaxAnswers(dataname):\n",
    "    return max([len(datasets[_dataName][datasets[_dataName][gameQuestion]==gameTitle]) for gameTitle in games])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(data20190603[idQuestion].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checks on the number of participants who played 3 games and provided complete answer forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for (_dataset, _datasetName) in zip([raw20190603, raw20190703, raw20190802, raw20190828], [\"raw20190603\", \"raw20190703\", \"raw20190802\", \"raw20190828\"]):\n",
    "#    print(_datasetName + \"\\n\" + str(_dataset[gameQuestion].value_counts()) + \"\\n\\n\")\n",
    "#    _dataset.loc[:,[idQuestion, gameQuestion]].sort_values([idQuestion, gameQuestion]).to_csv(dataFolderPath + \"IDs_&_Game_\" + _datasetName + csvSuffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that experimenters were excluded in filtering!\n",
    "\n",
    "\n",
    "# raw20190603\n",
    "# played 3 games:\n",
    "# played3 = [0, 1, 10, 12, 13, 14, 15, 16, 17, 18, 19, 2, 21, 22, 23, 24, 25, 26, 27, 28, 29, 3, 30, 31, 32, 33, 4, 5, 6, 7, 8, 9]\n",
    "# did not:\n",
    "# playedLess3 = [11, 20]\n",
    "\n",
    "# raw20190703\n",
    "# played 3 games:\n",
    "# played3 = [15, 20, 6, 8]\n",
    "# did not:\n",
    "# playedLess3 = [x for x in list(range(25)) if x not in played3]\n",
    "\n",
    "# raw20190828\n",
    "# played 3 games:\n",
    "# played3 = [3, 30, 31, 34, 4, 5, 8]\n",
    "# did not:\n",
    "# playedLess3 = [x for x in list(range(58)) if x not in played3]\n",
    "\n",
    "# raw20190802\n",
    "# played 3 games:\n",
    "# played3 = ?\n",
    "# did not:\n",
    "# playedLess3 = ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# comparison with data files ID lists\n",
    "for (_dataset, _datasetName) in zip([data20190603, data20190703, data20190802, data20190828], [\"data20190603\", \"data20190703\", \"data20190802\", \"data20190828\"]):\n",
    "    print(_datasetName)\n",
    "    print(str(list(np.unique(_dataset[idQuestion].values))))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diff between raw20190603's played 3 games and data20190603's IDs\n",
    "if False:\n",
    "    played3 = [0, 1, 10, 12, 13, 14, 15, 16, 17, 18, 19, 2, 21, 22, 23, 24, 25, 26, 27, 28, 29, 3, 30, 31, 32, 33, 4, 5, 6, 7, 8, 9]\n",
    "    indata = [0, 2, 3, 5, 6, 7, 8, 10, 13, 14, 15, 16, 17, 21, 23, 24, 25, 26, 28, 29, 30, 31, 32, 33]\n",
    "    print([x for x in played3 if x not in indata])\n",
    "    print([x for x in indata if x not in played3])\n",
    "# [1, 12, 18, 19, 22, 27, 4, 9]\n",
    "# []\n",
    "# remains to be accounted for: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allFaulty = []\n",
    "if True:\n",
    "    for column in indexedLikertQuestions:\n",
    "        faultyIDs = raw20190603.loc[raw20190603[column].isna()][idQuestion]\n",
    "        if len(faultyIDs) > 0:\n",
    "            #print(\"  allFaulty: \" + str(list(allFaulty)))\n",
    "            #print(\"+ faultyIDs: \" + str(list(faultyIDs.values)))\n",
    "            allFaulty = list(np.unique(allFaulty + list(faultyIDs.values)))\n",
    "            #print(\"= allFaulty: \" + str(list(allFaulty)))\n",
    "            #print()\n",
    "allFaulty"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
